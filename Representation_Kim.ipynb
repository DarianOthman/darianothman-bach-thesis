{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a1e9bb-2155-414d-9587-cb7d5e1d57c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/workspace/persistent/text_representation_kim.pkl', 'rb') as file:\n",
    "    # Load the contents of the PKL file\n",
    "    tk = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8041d446-94b9-435a-9082-1c1cee0a4136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/workspace/persistent/graph_representation_kim.pkl', 'rb') as file:\n",
    "    # Load the contents of the PKL file\n",
    "    gk = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f34c28e-b94b-403f-a751-bf97bb428b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "post_representation=[]\n",
    "for i in range(len(gk)):\n",
    "    concatenated_tensor = torch.cat([tk[i][0][0], gk[i][0][0]], dim=0)\n",
    "    post_representation.append(concatenated_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f770d35-d686-4480-a252-87a1262c8326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(770, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(64, 128)\n",
    "        self.fc4 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)  # Add a batch dimension\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc4(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "post_prediction=[]\n",
    "my_nn = Net()\n",
    "# Forward pass to get the output\n",
    "for i in range(len(post_representation)):\n",
    "    output = my_nn(post_representation[i]).argmax()\n",
    "    post_prediction.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b94ffdd-c377-4b01-9204-9e9a33c2843a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(post_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8550fe74-3a52-4376-b059-4de46ee058d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/workspace/persistent/post_prediction_kim.pkl', 'wb') as file:\n",
    "    pickle.dump(post_prediction, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e237f-89a6-453d-aaba-6579a03bffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/workspace/persistent/test_split_kim.pkl', 'rb') as file:\n",
    "    # Load the contents of the PKL file\n",
    "    test_split = pickle.load(file)\n",
    "test_split['id']=0\n",
    "test_split['id']=range(len(test_split))\n",
    "test_split = test_split.set_index(test_split['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b522c792-247b-4eef-bb6b-ba8b366569aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "true=[]\n",
    "pred=[]\n",
    "for i in range(len(test_split)):\n",
    "    true.append(float(test_split['sponsored'][i]))\n",
    "for i in range(len(test_split)):\n",
    "    pred.append(float(post_prediction[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8889e95-ba61-441f-9f49-c8c2027bd3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    " # Predicted values\n",
    "\n",
    "accuracy = accuracy_score(true, pred)\n",
    "precision = precision_score(true, pred)\n",
    "recall = recall_score(true, pred)\n",
    "f1 = f1_score(true, pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e7e338-c5cd-438c-a75c-042cde7f7136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "def listMLE_loss(pred_scores, true_scores):\n",
    "    pred_probs = softmax(pred_scores)\n",
    "    sorted_probs = np.sort(pred_probs)[::-1]\n",
    "    cum_probs = np.cumsum(sorted_probs)\n",
    "    loss = np.sum(np.log(1 + cum_probs)) - np.sum(np.log(1 + sorted_probs))\n",
    "    return loss\n",
    "\n",
    "def gradient_listMLE_loss(pred_scores, true_scores):\n",
    "    pred_probs = softmax(pred_scores)\n",
    "    sorted_probs = np.sort(pred_probs)[::-1]\n",
    "    cum_probs = np.cumsum(sorted_probs)\n",
    "    gradient = pred_probs - cum_probs / (1 + cum_probs)\n",
    "    return gradient\n",
    "\n",
    "def optimize_listMLE_loss(pred_scores, true_scores, learning_rate=0.1, num_iterations=100):\n",
    "    pred_scores = np.array(pred_scores)\n",
    "    true_scores = np.array(true_scores)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        gradient = gradient_listMLE_loss(pred_scores, true_scores)\n",
    "        pred_scores -= learning_rate * gradient\n",
    "\n",
    "    return pred_scores\n",
    "\n",
    "# Example usage\n",
    "pred_scores = pred\n",
    "true_scores = true\n",
    "\n",
    "optimized_scores = optimize_listMLE_loss(pred_scores, true_scores)\n",
    "loss = listMLE_loss(optimized_scores, true_scores)\n",
    "\n",
    "print(\"Optimized Scores:\", optimized_scores)\n",
    "print(\"ListMLE Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0421d32-e191-41d3-a3c7-bd173f6e321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(scores, truth, threshold):\n",
    "    rounded_scores = [0 if score <= threshold else 1 for score in scores]\n",
    "\n",
    "    tp = sum(1 for rounded_score, true_value in zip(rounded_scores, truth) if rounded_score == 1 and true_value == 1)\n",
    "    fp = sum(1 for rounded_score, true_value in zip(rounded_scores, truth) if rounded_score == 1 and true_value == 0)\n",
    "    fn = sum(1 for rounded_score, true_value in zip(rounded_scores, truth) if rounded_score == 0 and true_value == 1)\n",
    "\n",
    "    accuracy = sum(1 for rounded_score, true_value in zip(rounded_scores, truth) if rounded_score == true_value) / len(truth)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "accuracy, precision, recall, f1 = calculate_metrics(optimized_scores, true, 0.5)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
